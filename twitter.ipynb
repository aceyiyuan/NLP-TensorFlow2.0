{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "twitter.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "mount_file_id": "1zD_OkoW9louHXqIYJ5Gf37sH1u0eHDPb",
      "authorship_tag": "ABX9TyOCA4DBBWwXiHTykeK0mfgY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dizzySummer/NLP-TensorFlow2.0/blob/master/twitter.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2eyu7RLLn4rB",
        "colab_type": "code",
        "outputId": "c17f4d65-0789-435f-f626-b28e337b8382",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!pip install emoji"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: emoji in /usr/local/lib/python3.6/dist-packages (0.5.4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0BxdWEbQXzZx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorflow_version 2.x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mCdVJpErW0vd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "tf.keras.backend.clear_session()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7IojwpBwSLuh",
        "colab_type": "code",
        "outputId": "610920d7-afa3-474b-ed31-ce60024bcf76",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "#!pip install emoji\n",
        "\n",
        "import datetime\n",
        "import numpy as np \n",
        "import pandas as pd \n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Dense, Input\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "import tensorflow_hub as hub\n",
        "import emoji\n",
        "import os\n",
        "import sys\n",
        "#import fasttext\n",
        "import re\n",
        "import nltk \n",
        "nltk.download('punkt')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "import itertools\n",
        "import collections\n",
        "from collections import Counter\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud\n",
        "       "
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N8A6z55djfVk",
        "colab_type": "code",
        "outputId": "3b5b56be-d4c3-4b6b-eb0b-3f16d31db8fd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "import io\n",
        "\n",
        "df_train = pd.read_csv('/content/drive/My Drive/train.csv')\n",
        "df_test = pd.read_csv('/content/drive/My Drive/test.csv')\n",
        "submission = pd.read_csv('/content/drive/My Drive/sample_submission.csv')\n",
        "# Dataset is now stored in a Pandas Dataframe\n",
        "df_train.head()"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>keyword</th>\n",
              "      <th>location</th>\n",
              "      <th>text</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>5</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>All residents asked to 'shelter in place' are ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>6</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>7</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   id keyword  ...                                               text target\n",
              "0   1     NaN  ...  Our Deeds are the Reason of this #earthquake M...      1\n",
              "1   4     NaN  ...             Forest fire near La Ronge Sask. Canada      1\n",
              "2   5     NaN  ...  All residents asked to 'shelter in place' are ...      1\n",
              "3   6     NaN  ...  13,000 people receive #wildfires evacuation or...      1\n",
              "4   7     NaN  ...  Just got sent this photo from Ruby #Alaska as ...      1\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KyknbD4Kk31n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train=df_train[['text','target']]\n",
        "#null_data = train[train.isnull().any(axis=1)] there is no null value in test set after removing the id, keyword, location column\n",
        "test=df_test[['id','text']]\n",
        "pred=test"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_C8w8_-_lOMI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#import contractions list and remove it in the next step\n",
        "# A list of contractions from http://stackoverflow.com/questions/19790188/expanding-english-language-contractions-in-python\n",
        "contractions = { \n",
        "\"ain't\": \"am not\",\n",
        "\"aren't\": \"are not\",\n",
        "\"can't\": \"cannot\",\n",
        "\"can't've\": \"cannot have\",\n",
        "\"'cause\": \"because\",\n",
        "\"could've\": \"could have\",\n",
        "\"couldn't\": \"could not\",\n",
        "\"couldn't've\": \"could not have\",\n",
        "\"didn't\": \"did not\",\n",
        "\"doesn't\": \"does not\",\n",
        "\"don't\": \"do not\",\n",
        "\"hadn't\": \"had not\",\n",
        "\"hadn't've\": \"had not have\",\n",
        "\"hasn't\": \"has not\",\n",
        "\"haven't\": \"have not\",\n",
        "\"he'd\": \"he would\",\n",
        "\"he'd've\": \"he would have\",\n",
        "\"he'll\": \"he will\",\n",
        "\"he's\": \"he is\",\n",
        "\"how'd\": \"how did\",\n",
        "\"how'll\": \"how will\",\n",
        "\"how's\": \"how is\",\n",
        "\"i'd\": \"i would\",\n",
        "\"i'll\": \"i will\",\n",
        "\"i'm\": \"i am\",\n",
        "\"i've\": \"i have\",\n",
        "\"isn't\": \"is not\",\n",
        "\"it'd\": \"it would\",\n",
        "\"it'll\": \"it will\",\n",
        "\"it's\": \"it is\",\n",
        "\"let's\": \"let us\",\n",
        "\"ma'am\": \"madam\",\n",
        "\"mayn't\": \"may not\",\n",
        "\"might've\": \"might have\",\n",
        "\"mightn't\": \"might not\",\n",
        "\"must've\": \"must have\",\n",
        "\"mustn't\": \"must not\",\n",
        "\"needn't\": \"need not\",\n",
        "\"oughtn't\": \"ought not\",\n",
        "\"shan't\": \"shall not\",\n",
        "\"sha'n't\": \"shall not\",\n",
        "\"she'd\": \"she would\",\n",
        "\"she'll\": \"she will\",\n",
        "\"she's\": \"she is\",\n",
        "\"should've\": \"should have\",\n",
        "\"shouldn't\": \"should not\",\n",
        "\"that'd\": \"that would\",\n",
        "\"that's\": \"that is\",\n",
        "\"there'd\": \"there had\",\n",
        "\"there's\": \"there is\",\n",
        "\"they'd\": \"they would\",\n",
        "\"they'll\": \"they will\",\n",
        "\"they're\": \"they are\",\n",
        "\"they've\": \"they have\",\n",
        "\"wasn't\": \"was not\",\n",
        "\"we'd\": \"we would\",\n",
        "\"we'll\": \"we will\",\n",
        "\"we're\": \"we are\",\n",
        "\"we've\": \"we have\",\n",
        "\"weren't\": \"were not\",\n",
        "\"what'll\": \"what will\",\n",
        "\"what're\": \"what are\",\n",
        "\"what's\": \"what is\",\n",
        "\"what've\": \"what have\",\n",
        "\"where'd\": \"where did\",\n",
        "\"where's\": \"where is\",\n",
        "\"who'll\": \"who will\",\n",
        "\"who's\": \"who is\",\n",
        "\"won't\": \"will not\",\n",
        "\"wouldn't\": \"would not\",\n",
        "\"you'd\": \"you would\",\n",
        "\"you'll\": \"you will\",\n",
        "\"you're\": \"you are\",\n",
        "\"thx\"   : \"thanks\"\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-kQKSdp0laS3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def remove_contractions(text):\n",
        "    return contractions[text.lower()] if text.lower() in contractions.keys() else text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S2HJ-RjWlfAq",
        "colab_type": "code",
        "outputId": "b980566a-9d1f-4b9c-c8be-c108fab2e284",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "train['text']=train['text'].apply(remove_contractions)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j9EmfKYTllMR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#the original test dataset which does not have labels\n",
        "pred=pred['text'].apply(remove_contractions)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z0MMWy_xlq9b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# clean dataset\n",
        "def clean_dataset(text):\n",
        "    # To lowercase\n",
        "    text = text.lower()\n",
        "    # Remove hashtag while keeping hashtag text\n",
        "    text = re.sub(r'#','', text)\n",
        "    # Remove HTML special entities (e.g. &amp;)\n",
        "    text = re.sub(r'\\&\\w*;', '', text)\n",
        "    # Remove tickers\n",
        "    text = re.sub(r'\\$\\w*', '', text)\n",
        "    # Remove hyperlinks\n",
        "    text = re.sub(r'https?:\\/\\/.*\\/\\w*', '', text)\n",
        "    # Remove whitespace (including new line characters)\n",
        "    text = re.sub(r'\\s\\s+','', text)\n",
        "    text = re.sub(r'[ ]{2, }',' ',text)\n",
        "    # Remove URL, RT, mention(@)\n",
        "    text=  re.sub(r'http(\\S)+', '',text)\n",
        "    text=  re.sub(r'http ...', '',text)\n",
        "    text=  re.sub(r'(RT|rt)[ ]*@[ ]*[\\S]+','',text)\n",
        "    text=  re.sub(r'RT[ ]?@','',text)\n",
        "    text = re.sub(r'@[\\S]+','',text)\n",
        "    # Remove words with 2 or fewer letters\n",
        "    text = re.sub(r'\\b\\w{1,2}\\b', '', text)\n",
        "    #&, < and >\n",
        "    text = re.sub(r'&amp;?', 'and',text)\n",
        "    text = re.sub(r'&lt;','<',text)\n",
        "    text = re.sub(r'&gt;','>',text)\n",
        "    # Insert space between words and punctuation marks\n",
        "    text = re.sub(r'([\\w\\d]+)([^\\w\\d ]+)', '\\1 \\2',text)\n",
        "    text = re.sub(r'([^\\w\\d ]+)([\\w\\d]+)', '\\1 \\2',text)\n",
        "    # Remove characters beyond Basic Multilingual Plane (BMP) of Unicode:\n",
        "    text= ''.join(c for c in text if c <= '\\uFFFF') \n",
        "    text = text.strip()\n",
        "    # Remove misspelling words\n",
        "    text = ''.join(''.join(s)[:2] for _, s in itertools.groupby(text))\n",
        "    # Remove punctuation\n",
        "    text = ' '.join(re.sub(\"[\\.\\,\\!\\?\\:\\;\\-\\=\\/\\|\\'\\(\\']\", \" \", text).split())\n",
        "    # Remove emoji\n",
        "    text = emoji.demojize(text)\n",
        "    text = text.replace(\":\",\" \")\n",
        "    text = ' '.join(text.split()) \n",
        "    text = re.sub(\"([^\\x00-\\x7F])+\",\" \",text)\n",
        "    # Remove Mojibake (also extra spaces)\n",
        "    text = ' '.join(re.sub(\"[^\\u4e00-\\u9fa5\\u0030-\\u0039\\u0041-\\u005a\\u0061-\\u007a]\", \" \", text).split())\n",
        "    return text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "32eTC3JLlszy",
        "colab_type": "code",
        "outputId": "2973c9f0-b1d0-48e9-c4bd-046e6eccf60d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "train['text'] =train['text'].apply(clean_dataset)"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IygiCPE7l1Ip",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pred=pred.apply(clean_dataset)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PYnEuO-cl5hi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# I ended up with building my own stop word lists because NLTP simply removed all negation words which totally changed the meaning of the sentence. \n",
        "myOwnStopList=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'what','how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'do', 'should', \"should\", 'now', 'd', 'm', 'o', 're', 've', 'y', 'ain', \"are\", 'could', \"was\",\n",
        "'would','have','get','got','getting','one','two','still','going']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DjOMvsCkl9nk",
        "colab_type": "code",
        "outputId": "17632f6e-22b5-4805-f979-8be25bbf62f7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "#stop = stopwords.words('english')\n",
        "#stop+=['get','got','getting','one','two','would','still','could','going']#customized stop word list\n",
        "#stop = [e for e in stop if e not in (\"n't\", \"not\", \"no\")]\n",
        "\n",
        "# Exclude stopwords with Python's list comprehension and pandas.DataFrame.apply.\n",
        "\n",
        "train['text'] = train['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (myOwnStopList)]))"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FvHgRxEsmCAd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pred=pred.apply(lambda x: ' '.join([word for word in x.split() if word not in (myOwnStopList)]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DvS7xEjqmIMT",
        "colab_type": "code",
        "outputId": "ebb4ee1c-30b6-4aa9-cb99-7e4303462987",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!pip --no-cache-dir install tokenizers==0.4.2 #you get only a warning, just ignore it"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tokenizers==0.4.2 in /usr/local/lib/python3.6/dist-packages (0.4.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qoEUIiSymSct",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!pip install tokenizers #hugging face tokenizer\n",
        "from tokenizers import (ByteLevelBPETokenizer,\n",
        "                            CharBPETokenizer,\n",
        "                            SentencePieceBPETokenizer,\n",
        "                            BertWordPieceTokenizer)\n",
        "tokenizer = BertWordPieceTokenizer()\n",
        "\n",
        "path=\"/content/drive/My Drive/tobeTrained.txt\"\n",
        "#set vocab_size to 15000 as the len(train_set)was something like 12500 \n",
        "tokenizer.train(files=path, vocab_size=15_000, min_frequency=2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JR7WF3s6mfYJ",
        "colab_type": "code",
        "outputId": "7edcd778-3a5b-404c-cf45-f7dfc093292a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "tokenizer.save(\".\", \"/content/drive/My Drive/newBert\")"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/content/drive/My Drive/newBert-vocab.txt']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zayXA48VmxQf",
        "colab_type": "code",
        "outputId": "b18815a5-c98a-476a-bc07-fad41736b6f9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#token.ids  PAD-->0, UNK-->1, CLS-->2 (start of sentence),SEP-->3 (end of sentence),MASK-->4\n",
        "tokenizer = BertWordPieceTokenizer(\n",
        "    \"/content/drive/My Drive/newBert-vocab.txt\",\n",
        "    lowercase=True, \n",
        "    unk_token=\"<unk>\",\n",
        "    sep_token=\"</s>\",\n",
        "    cls_token=\"<s>\",\n",
        ")\n",
        "\n",
        "tokenizer.add_special_tokens([\"<pad>\", \"<mask>\"])# same as default, unnecessary?"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CiOnvLBUnnJN",
        "colab_type": "code",
        "outputId": "68e7fe79-4d66-4de2-9dfd-acb6e3c7af29",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "print(tokenizer)"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tokenizer(vocabulary_size=9938, model=BertWordPiece, add_special_tokens=True, unk_token=<unk>, sep_token=</s>, cls_token=<s>, clean_text=True, handle_chinese_chars=True, strip_accents=True, lowercase=True, wordpieces_prefix=##)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KtWalHw-n20H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#output = tokenizer.encode(press2[0])\n",
        "#print(output.tokens)\n",
        "#print(output.ids)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JvbOMmUCofI_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# As we don't have label for test dataset, I will split train dataset as trainsub and testsub dataset.\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_subtrain, X_subtest, y_subtrain, y_subtest = train_test_split(\n",
        "  train['text'],train['target'], test_size=0.2, random_state=42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xet2maeixCD0",
        "colab_type": "code",
        "outputId": "ef7eb2ef-92ba-4e55-8222-d3cd04d02d96",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "print(X_subtrain)"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4996    courageous honest analysis need use atomic bom...\n",
            "3263     wld shame golf cart became engulfed boycottbears\n",
            "4907    tell rescind medals given soldiers massacre wo...\n",
            "2855          worried drought might affect extreme dampen\n",
            "4716                    lava blastpower red pantherattack\n",
            "                              ...                        \n",
            "5226           many obliteration servers always like play\n",
            "5390    panic attack enough money drugs alcohol wanted...\n",
            "860     omron automatic blood pressure monitor standar...\n",
            "7603    officials say quarantine place alabama home po...\n",
            "7270          moved england five years ago whirlwind time\n",
            "Name: text, Length: 6090, dtype: object\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bQIATDHr1Oa4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def bert_token(texts,max_len=128): \n",
        "    all_input_ids=[]\n",
        "    all_mask_ids=[]\n",
        "    all_seg_ids=[]\n",
        "    for token in texts: \n",
        "    \n",
        "        input_ids=tokenizer.encode(token).ids\n",
        "        mask_ids = [1] * len(input_ids)\n",
        "        seg_ids = [0] * len(input_ids)\n",
        "        padding = [0] * (max_len - len(input_ids))\n",
        "        input_ids += padding\n",
        "        mask_ids += padding\n",
        "        seg_ids += padding\n",
        "        all_input_ids.append(input_ids)\n",
        "        all_mask_ids.append(mask_ids)\n",
        "        all_seg_ids.append(seg_ids)\n",
        "\n",
        "    \n",
        "    return np.array(all_input_ids), np.array(all_mask_ids), np.array(all_seg_ids)\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KaSeT8XY89M7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_input=bert_token(X_subtrain,max_len=32)\n",
        "test_input=bert_token(X_subtest,max_len=32)\n",
        "pred=bert_token(pred,max_len=32)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7NEXJk79Y2Fm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_model(bert_layer, max_len=128):\n",
        "    input_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_ids\")\n",
        "    mask_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"mask_ids\")\n",
        "    seg_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"seg_ids\")\n",
        "\n",
        "    pooled_output, sequence_output = bert_layer([input_ids,mask_ids,  seg_ids])\n",
        "    clf_output = sequence_output[:, 0, :]\n",
        "    out = Dense(1, activation='sigmoid')(clf_output)\n",
        "    \n",
        "    model = Model(inputs=[input_ids,  mask_ids,  seg_ids], outputs=out)\n",
        "    model.compile(Adam(lr=2e-6), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    \n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OgVjznbganiE",
        "colab_type": "code",
        "outputId": "3f8d0136-fff9-4403-9492-03cc9b12c120",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "%%time\n",
        "\n",
        "module_url = \"https://tfhub.dev/tensorflow/bert_en_wwm_uncased_L-24_H-1024_A-16/1\"\n",
        "bert_layer = hub.KerasLayer(module_url, trainable=True)"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 9.21 s, sys: 1.53 s, total: 10.7 s\n",
            "Wall time: 10.6 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o1Zu-DJ9dsMN",
        "colab_type": "code",
        "outputId": "0387c6d6-3944-4718-ee19-862eead418a3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        }
      },
      "source": [
        "model = build_model(bert_layer, max_len=32)\n",
        "model.summary()"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_ids (InputLayer)          [(None, 32)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "mask_ids (InputLayer)           [(None, 32)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "seg_ids (InputLayer)            [(None, 32)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "keras_layer (KerasLayer)        [(None, 1024), (None 335141889   input_ids[0][0]                  \n",
            "                                                                 mask_ids[0][0]                   \n",
            "                                                                 seg_ids[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_strided_slice (Tens [(None, 1024)]       0           keras_layer[0][1]                \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 1)            1025        tf_op_layer_strided_slice[0][0]  \n",
            "==================================================================================================\n",
            "Total params: 335,142,914\n",
            "Trainable params: 335,142,913\n",
            "Non-trainable params: 1\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pN4nhVmLqNkc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Can I use TensorBoard with Google Colab?\n",
        "#Tensorboard: ValueError: Duplicate plugins for name projector #22676\n",
        "#https://github.com/pytorch/pytorch/issues/22676\n",
        "\n",
        "logs_base_dir = \"./logs\"\n",
        "logdir = os.path.join(logs_base_dir, datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZUDTMhCzbCBp",
        "colab_type": "code",
        "outputId": "e30fcac2-c720-401a-fefe-b0a80fb645d7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 731
        }
      },
      "source": [
        "model.fit(\n",
        "    train_input, \n",
        "    y_subtrain,\n",
        "    callbacks=[tensorboard_callback],\n",
        "    validation_data=(test_input, y_subtest),\n",
        "    epochs=20,\n",
        "    batch_size=10\n",
        ")"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 6090 samples, validate on 1523 samples\n",
            "Epoch 1/20\n",
            "6090/6090 [==============================] - 128s 21ms/sample - loss: 0.6716 - accuracy: 0.5952 - val_loss: 0.6715 - val_accuracy: 0.5660\n",
            "Epoch 2/20\n",
            "6090/6090 [==============================] - 100s 16ms/sample - loss: 0.4823 - accuracy: 0.7816 - val_loss: 0.6123 - val_accuracy: 0.6888\n",
            "Epoch 3/20\n",
            "6090/6090 [==============================] - 100s 16ms/sample - loss: 0.4809 - accuracy: 0.7479 - val_loss: 0.7527 - val_accuracy: 0.5844\n",
            "Epoch 4/20\n",
            "6090/6090 [==============================] - 100s 16ms/sample - loss: 0.3021 - accuracy: 0.8764 - val_loss: 0.7497 - val_accuracy: 0.6573\n",
            "Epoch 5/20\n",
            "6090/6090 [==============================] - 100s 16ms/sample - loss: 0.1478 - accuracy: 0.9509 - val_loss: 0.7790 - val_accuracy: 0.6927\n",
            "Epoch 6/20\n",
            "6090/6090 [==============================] - 100s 16ms/sample - loss: 0.1257 - accuracy: 0.9586 - val_loss: 0.8813 - val_accuracy: 0.7065\n",
            "Epoch 7/20\n",
            "6090/6090 [==============================] - 100s 16ms/sample - loss: 0.0999 - accuracy: 0.9677 - val_loss: 0.8530 - val_accuracy: 0.6868\n",
            "Epoch 8/20\n",
            "6090/6090 [==============================] - 100s 16ms/sample - loss: 0.0823 - accuracy: 0.9714 - val_loss: 0.9098 - val_accuracy: 0.6822\n",
            "Epoch 9/20\n",
            "6090/6090 [==============================] - 100s 16ms/sample - loss: 0.0768 - accuracy: 0.9732 - val_loss: 0.9250 - val_accuracy: 0.6717\n",
            "Epoch 10/20\n",
            "6090/6090 [==============================] - 100s 16ms/sample - loss: 0.0698 - accuracy: 0.9722 - val_loss: 0.9728 - val_accuracy: 0.7052\n",
            "Epoch 11/20\n",
            "6090/6090 [==============================] - 100s 16ms/sample - loss: 0.0651 - accuracy: 0.9732 - val_loss: 0.9956 - val_accuracy: 0.7019\n",
            "Epoch 12/20\n",
            "6090/6090 [==============================] - 100s 16ms/sample - loss: 0.0616 - accuracy: 0.9739 - val_loss: 0.9544 - val_accuracy: 0.6809\n",
            "Epoch 13/20\n",
            "6090/6090 [==============================] - 100s 16ms/sample - loss: 0.0565 - accuracy: 0.9729 - val_loss: 1.0625 - val_accuracy: 0.6789\n",
            "Epoch 14/20\n",
            "6090/6090 [==============================] - 100s 16ms/sample - loss: 0.0580 - accuracy: 0.9732 - val_loss: 0.9317 - val_accuracy: 0.6796\n",
            "Epoch 15/20\n",
            "6090/6090 [==============================] - 100s 16ms/sample - loss: 0.0640 - accuracy: 0.9718 - val_loss: 1.0693 - val_accuracy: 0.6796\n",
            "Epoch 16/20\n",
            "6090/6090 [==============================] - 100s 16ms/sample - loss: 0.0505 - accuracy: 0.9736 - val_loss: 1.2152 - val_accuracy: 0.6868\n",
            "Epoch 17/20\n",
            "6090/6090 [==============================] - 100s 16ms/sample - loss: 0.0550 - accuracy: 0.9722 - val_loss: 1.2351 - val_accuracy: 0.6914\n",
            "Epoch 18/20\n",
            "6090/6090 [==============================] - 100s 16ms/sample - loss: 0.0513 - accuracy: 0.9741 - val_loss: 1.2673 - val_accuracy: 0.6967\n",
            "Epoch 19/20\n",
            "6090/6090 [==============================] - 100s 16ms/sample - loss: 0.0558 - accuracy: 0.9724 - val_loss: 1.1915 - val_accuracy: 0.6763\n",
            "Epoch 20/20\n",
            "6090/6090 [==============================] - 100s 16ms/sample - loss: 0.0431 - accuracy: 0.9777 - val_loss: 1.3331 - val_accuracy: 0.7058\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fdcc0341278>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "boRItaH8dodU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.save('BertModel.h5')\n",
        "pred = model.predict(pred)\n",
        "model.load_weights('BertModel.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YvipLPgIeY9H",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "f9926649-0024-4753-b517-1b56db24a664"
      },
      "source": [
        "submission['target'] =pred.round().astype(int)\n",
        "print(submission['target'])"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0       0\n",
            "1       1\n",
            "2       0\n",
            "3       0\n",
            "4       1\n",
            "       ..\n",
            "3258    1\n",
            "3259    0\n",
            "3260    0\n",
            "3261    0\n",
            "3262    0\n",
            "Name: target, Length: 3263, dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OE16PzuTfgCo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "05b635b2-4f24-43df-a557-aef74e92bf10"
      },
      "source": [
        "!kill 936"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/bin/bash: line 0: kill: (936) - No such process\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j8srpBnuqtxX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 872
        },
        "outputId": "0c9c7258-09c6-40e9-e84a-3564f469dd09"
      },
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir logs"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The tensorboard extension is already loaded. To reload it, use:\n",
            "  %reload_ext tensorboard\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Reusing TensorBoard on port 6006 (pid 936), started 1:29:46 ago. (Use '!kill 936' to kill it.)"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "  (async () => {\n",
              "      const url = await google.colab.kernel.proxyPort(6006, {\"cache\": true});\n",
              "      const iframe = document.createElement('iframe');\n",
              "      iframe.src = url;\n",
              "      iframe.setAttribute('width', '100%');\n",
              "      iframe.setAttribute('height', '800');\n",
              "      iframe.setAttribute('frameborder', 0);\n",
              "      document.body.appendChild(iframe);\n",
              "  })();\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}